{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZJsH6GX3LcU"
   },
   "source": [
    "## \"journal pacifique\" web bot\n",
    "parses sources for new articles, translates them into target languages [\"es\",\"pt\",\"pl\",\"zh-CN\"]. Wraps it up in a simple html document and writes it out to the appropriate directory.\n",
    "\n",
    "#### TO DO TECHNICAL:\n",
    "\n",
    "*   save the articles dictionary as a json or something (__done__)\n",
    "*   *   (dic backup for later design changes) important\n",
    "*   implement an automatic parser (__done(sd)__)\n",
    "*   detect and highlight keywords\n",
    "*   automatic git push (__done__ but pass cache time should be increased)\n",
    "*   run the script on an aws server\n",
    "*   maybe an open source grammar rating mechanism?\n",
    "*   don't send a separate request to the website for every translation (__done__)\n",
    "*   same pages shouldn't be crawled twice. Even if they are crawled they shouldn't be overwritten. (__done__)\n",
    "*   implement waiting if requests exhaust limit\n",
    "*   also implement a timeout. maybe take articles into a queue.\n",
    "*   re-read SD's [terms](https://www.sciencedaily.com/terms.htm) on reproduction. (especially on images)\n",
    "*   take the bot 1 directory up so it won't be pushed into the public repo. (__done__)\n",
    "\n",
    "#### TO DO GRAPHICAL\n",
    "*   responsive text, currently changes size by page??\n",
    "*   resizable design (__done__)\n",
    "*   mobile-friendly design\n",
    "*   *   [media queries for different device widths](https://stackoverflow.com/questions/16387400/getting-the-right-font-size-on-every-mobile-device)\n",
    "*   subscribe and social media block\n",
    "\n",
    "\n",
    "#### POSSIBLE SOURCES\n",
    "*   [the conversation](https://theconversation.com/uk/republishing-guidelines)\n",
    "*   *   approval needed for translation\n",
    "*   \n",
    "\n",
    "NOTE: google transalte tokens refresh every hour (?)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "am1FAv62IYw-",
    "outputId": "a6430627-a1b7-4973-b011-c8db27c4d236"
   },
   "outputs": [],
   "source": [
    "#for colab\n",
    "#!pip install googletrans\n",
    "#!pip install mechanize\n",
    "\n",
    "## Character limit: 15K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#cmd = \"cd cemreefe.github.io; git pull; cd ..\"\n",
    "#os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcmCkaflIcmH"
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import re\n",
    "\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSj3rydyWNvr"
   },
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gg1MHPIoPIKG"
   },
   "outputs": [],
   "source": [
    "def string_strip(s):\n",
    "    return re.sub(r\"[^A-Za-z0-9]\", \"-\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cFY6cTtOwLM"
   },
   "outputs": [],
   "source": [
    "#def sciencedaily_parse_links():\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-z6efhTaAfI"
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from random import randrange\n",
    "\n",
    "def sciencedaily_parse_article(url):\n",
    "  \n",
    "    site = url\n",
    "    html_string = \"\"\n",
    "    tagged_w_ps = \"\"\n",
    "\n",
    "    # parse html\n",
    "\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    html_string = webpage.decode(\"utf-8\") \n",
    "  \n",
    "    # get info\n",
    "  \n",
    "    # get the headline\n",
    "\n",
    "    headline = find_between(html_string, '<h1 id=\"headline\" class=\"headline\">','</h1>')\n",
    "  \n",
    "    # get the meta\n",
    "  \n",
    "    meta     = find_between(html_string, '<dd id=\"abstract\">','</dd>')\n",
    "  \n",
    "    # get the img address\n",
    "  \n",
    "    imgaddr  = find_between(html_string, 'center-block\" src=\"','\"')\n",
    "    \n",
    "    # set imgaddr image from sciencedaily if it exists. Else use one of the dummy imgs.\n",
    "    \n",
    "    if(imgaddr!=\"\"):\n",
    "        imgaddr  = 'https://www.sciencedaily.com'+imgaddr\n",
    "    else:\n",
    "        imgaddr  = '../../../img/dummy'+repr(randrange(20))+'.jpeg'\n",
    "    \n",
    "    # get random image from dummies since sciencedaily images get dead links later on.\n",
    "    #imgaddr  = '../../../img/dummy'+repr(randrange(20))+'.jpeg'\n",
    "  \n",
    "    # get the img alt\n",
    "  \n",
    "    imgalt   = find_between(html_string, '<div class=\"photo-caption\">','</div>')\n",
    "    cred     = '\\t'+find_between(html_string, '<div class=\"photo-credit\"><em>','</div>')\n",
    "  \n",
    "    # get the citation\n",
    "  \n",
    "    citation = find_between(html_string, '<div role=\"tabpanel\" class=\"tab-pane active\" id=\"citation_mla\">','</div>')\n",
    "  \n",
    "    \n",
    "    # get the article itself\n",
    "\n",
    "    article  = find_between(html_string, '<div id=\"text\">','</div>')\n",
    "    article  = re.sub(r\"<p>\" ,     \"\", article)\n",
    "    article  = re.sub(r\"</p>\", \"\\n\", article)\n",
    "  \n",
    "    # get the href at the end if it exists\n",
    "    href_tab = \"\"\n",
    "    href_tab = href_tab + find_between(article, '<a href','</a>')\n",
    "  \n",
    "    # delete the href and useless tags from the article\n",
    "  \n",
    "    article  = re.sub(r\"<a href.*?</a>\", \"\", article)\n",
    "    article  = re.sub(r\"<.*?>\", \"\",          article)\n",
    "    article  = re.sub(r\" -- \", \", \",         article)\n",
    "  \n",
    "    # special characters, this list will expand.\n",
    "  \n",
    "    #article  = re.sub(r\"&uuml;\", \"ü\",         article)\n",
    "    #article  = re.sub(r\"&ouml;\", \"ö\",         article)\n",
    "    #article  = re.sub(r\"&eacute;\", \"e\",         article)\n",
    "    # new sltn:\n",
    "    import html\n",
    "    article   = html.unescape(article)\n",
    "      \n",
    "            #y                    #y                                #y                                                       #y                  \n",
    "    return {\"headline\": headline, \"meta\": meta, \"imgaddr\": imgaddr, \"imgalt\": imgalt, \"imgcredit\":cred,\"citation\": citation, \"article\": article, \"href\": href_tab}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9bqtXa0mCjf"
   },
   "outputs": [],
   "source": [
    "def get_translation(article_dictionary,target_language):\n",
    "    big_string= \"\"\n",
    "    translations = {}\n",
    "    token_in = \"(#@)\"\n",
    "    if (target_language==\"zh-CN\"):\n",
    "        token_out = \"（＃@）\"\n",
    "    else: token_out=\"(# @)\"\n",
    "      \n",
    "    \n",
    "    #sırası karışmasın!! keylerden array oluştur, arrayi itere et.\n",
    "    keys = [\"headline\",\"meta\",\"article\",\"imgalt\"]\n",
    "    \n",
    "    for key in keys:\n",
    "        print(key)\n",
    "        if big_string != \"\":\n",
    "            big_string = big_string + token_in + article_dictionary[key]\n",
    "        else:\n",
    "            big_string = article_dictionary[key]\n",
    "            \n",
    "    big_translated_string = translator.translate(big_string,dest=target_language).text\n",
    "    \n",
    "    translations_array    = big_translated_string.split(token_out)\n",
    "   \n",
    "    #print(big_translated_string)\n",
    "    #print(len(translations_array),translations_array)\n",
    "    \n",
    "    for i in range (4):\n",
    "        translations[keys[i]]=translations_array[i]\n",
    "    translations[\"imgaddr\"]=article_dictionary[\"imgaddr\"]\n",
    "    translations[\"citation\"]=article_dictionary[\"citation\"]\n",
    "    translations[\"imgcredit\"]=article_dictionary[\"imgcredit\"]\n",
    "    translations[\"href\"]=article_dictionary[\"href\"]\n",
    "    translations[\"org-headline\"]=article_dictionary[\"headline\"]\n",
    "    translations[\"lang\"]=target_language\n",
    "    \n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcFIKve4csNY"
   },
   "outputs": [],
   "source": [
    ";# deprecated\n",
    "def get_language_dictionary(target_language):\n",
    "    ld = {}\n",
    "    words = [\"journal pacifique\", \"homepage\", \"archive\", \"about us\", \"source:\"]\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        ld[word] = translator.translate(word, dest=target_language).text\n",
    "    return ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSvSx7ACTufo"
   },
   "outputs": [],
   "source": [
    "def html_from_dictionary(translated_dictionary, target_language, language_dictionary): # translated & language could be just one dictionary this was stupid.\n",
    "  \n",
    "    from datetime import date\n",
    "    hoy  = date.today()\n",
    "    d1   = hoy.strftime(\"%d/%m/%Y\")\n",
    "  \n",
    "    html = open(SUBFOLDER + \"jp.temp\").read()\n",
    "\n",
    "    html_article = re.sub(r\"\\n\\n\", \"</p>\\n\\n<p>\",   translated_dictionary[\"article\"])\n",
    "    html_article = \"<p>\" + html_article + \"</p>\"\n",
    "\n",
    "    print(translated_dictionary[\"headline\"])\n",
    "  \n",
    "    html = re.sub(r\"\\$\\$article-title%%\",  translated_dictionary[\"headline\"],         html)\n",
    "    html = re.sub(r\"\\$\\$img-alt%%\",        translated_dictionary[\"imgalt\"],           html)\n",
    "    html = re.sub(r\"\\$\\$article-meta%%\",   translated_dictionary[\"meta\"],             html)\n",
    "    html = re.sub(r\"\\$\\$source%%\",         translated_dictionary[\"citation\"],            html)\n",
    "    html = re.sub(r\"\\$\\$img.jpg%%\",        translated_dictionary[\"imgaddr\"],             html)\n",
    "    html = re.sub(r\"\\$\\$article-text%%\",   html_article                  ,            html)\n",
    "  \n",
    "    html = re.sub(r\"\\$\\$home%%\",           language_dictionary[\"homepage-\"+target_language],           html)\n",
    "    html = re.sub(r\"\\$\\$archive%%\",        language_dictionary[\"archive-\"+target_language],            html)\n",
    "    html = re.sub(r\"\\$\\$about%%\",          language_dictionary[\"about us-\"+target_language],           html)\n",
    "    html = re.sub(r\"\\$\\$jp-translation%%\", language_dictionary[\"journal pacifique-\"+target_language],  html)\n",
    "    html = re.sub(r\"\\$\\$taken-from%%\",     language_dictionary[\"source:-\"+target_language],            html)\n",
    "    html = re.sub(r\"\\$\\$target-languages%%\",     target_language,            html)\n",
    "  \n",
    "    html = re.sub(r\"\\$\\$imgcredit%%\",       translated_dictionary[\"imgcredit\"],          html)\n",
    "  \n",
    "    html = re.sub(r\"\\$\\$href%%\",           translated_dictionary[\"href\"],            html)\n",
    "  \n",
    "    html = re.sub(r\"-- \", \",\",  html)\n",
    " \n",
    "    html = re.sub(r\"\\$\\$date%%\", d1, html)\n",
    "  \n",
    "    #html = re.sub(r'</head>', '<style> p { text-indent: 30px;} </style>\\n\\t</head>', html)\n",
    "  \n",
    "    return html\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8_T668O03h9"
   },
   "outputs": [],
   "source": [
    "def getnewpath(translated_dict):\n",
    "  \n",
    "    from datetime import date\n",
    "    hoy  = date.today()\n",
    "    date = hoy.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    translated_dict[\"date\"]=date\n",
    "  \n",
    "    urlheadline = string_strip(translated_dict[\"org-headline\"])+\"-\"+translated_dict[\"lang\"]\n",
    "    postsfolder = \"/posts/\"+date+\"/\"\n",
    "    newpathaddr = translated_dict[\"lang\"]+postsfolder\n",
    "    newheadline = newpathaddr+urlheadline+\".html\"\n",
    "    newheadlang = postsfolder+urlheadline+\".html\"\n",
    "    \n",
    "    translated_dict[\"pathfromhome\"]=newheadline\n",
    "    translated_dict[\"pathfromlang\"]=newheadlang\n",
    "    \n",
    "    # this was implemented for short headlines (~10char)\n",
    "    #if newheadline in headlinessofar:\n",
    "    #    c=2\n",
    "    #    while (newheadline[:-5]+repr(c)+newheadline[-5:] in headlinessofar):\n",
    "    #        c+=1\n",
    "    #    newheadline = newheadline[:-5]+repr(c)+newheadline[-5:]\n",
    "    \n",
    "    headlinessofar.append(newheadline)\n",
    "    return [newheadline,newpathaddr,urlheadline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBFOLDER = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HD6jZB1jmpVe"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(SUBFOLDER+'obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(SUBFOLDER+'obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5dlG_rpmpVm"
   },
   "outputs": [],
   "source": [
    "def dic_to_dirfile(dic,target_language, elements_dictionary):\n",
    "    \n",
    "    #we used to translate all elements at every step, now it is automated due to\n",
    "    #google translate's quota restrictions.\n",
    "    \n",
    "    #zh_ld = get_language_dictionary(target_language)\n",
    "\n",
    "    tr_di = get_translation(dic,target_language)\n",
    "    htmlx = html_from_dictionary(tr_di, target_language, elements_dictionary)\n",
    "  \n",
    "    newdirs = getnewpath(tr_di)\n",
    "  \n",
    "    import os\n",
    "    cmd = \"mkdir -p \"+SUBFOLDER+newdirs[1]\n",
    "    os.system(cmd)\n",
    "  \n",
    "    f= open(SUBFOLDER+newdirs[0],\"w+\")\n",
    "    f.write(htmlx)\n",
    "\n",
    "    return [htmlx,tr_di,newdirs[2],newdirs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targdic_to_dirfile(tr_di,target_language, elements_dictionary):\n",
    "    \n",
    "    htmlx = html_from_dictionary(tr_di, target_language, elements_dictionary)\n",
    "  \n",
    "    newdirs = getnewpath(tr_di)\n",
    "  \n",
    "    import os\n",
    "    cmd = \"mkdir -p \"+SUBFOLDER+newdirs[1]\n",
    "    os.system(cmd)\n",
    "  \n",
    "    f= open(SUBFOLDER+newdirs[0],\"w+\")\n",
    "    f.write(htmlx)\n",
    "    \n",
    "    print(SUBFOLDER+newdirs[0])\n",
    "  \n",
    "    return [htmlx,tr_di,newdirs[2],newdirs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7W8jsdDvoxH"
   },
   "outputs": [],
   "source": [
    "elements_dictionary = {\n",
    "    \"journal pacifique-es\":\"periódico pacífico\", \n",
    "    \"homepage-es\":\"página principal\", \n",
    "    \"archive-es\":\"archivo\",\n",
    "    \"about us-es\":\"sobre nosotros\",\n",
    "    \"source:-es\":\"fuente:\",\n",
    "    \"journal pacifique-pt\":\"jornal pacífico\", \n",
    "    \"homepage-pt\":\"pagina inicial\", \n",
    "    \"archive-pt\":\"arquivo\",\n",
    "    \"about us-pt\":\"sobre nós\",\n",
    "    \"source:-pt\":\"fonte:\",\n",
    "    \"journal pacifique-tr\":\"barışçıl gazete\", \n",
    "    \"homepage-tr\":\"anasayfa\", \n",
    "    \"archive-tr\":\"arşiv\",\n",
    "    \"about us-tr\":\"hakkında\",\n",
    "    \"source:-tr\":\"kaynakça:\",\n",
    "    \"journal pacifique-zh-CN\":\"和平的报纸\", \n",
    "    \"homepage-zh-CN\":\"主页\", \n",
    "    \"archive-zh-CN\":\"档案\",\n",
    "    \"about us-zh-CN\":\"关于我们\",\n",
    "    \"source:-zh-CN\":\"资源\",\n",
    "    \"journal pacifique-pl\":\"spokojna gazeta\", \n",
    "    \"homepage-pl\":\"strona główna\", \n",
    "    \"archive-pl\":\"archiwum\",\n",
    "    \"about us-pl\":\"o nas\",\n",
    "    \"source:-pl\":\"źródło\",\n",
    "    \"latest-articles-es\":\"Últimos artículos\",\n",
    "    \"latest-articles-pt\":\"Artigos Mais Recentes\",\n",
    "    \"latest-articles-pl\":\"Ostatnie artykuły\",\n",
    "    \"latest-articles-zh-CN\":\"最新的文章\",\n",
    "    \"latest_articles-tr\":\"En yeni makaleler\",\n",
    "    \"hometext-es\":\"Bienvenido a Journal Pacifique. Le proporcionamos los últimos artículos sobre ciencia y tecnología. Journal Pacifique se dedica a la distribución de investigaciones científicas populares en otros idiomas además del inglés.\",\n",
    "    \"hometext-pt\":\"Bem-vindo ao Journal Pacifique. Fornecemos os artigos mais recentes sobre ciência e tecnologia. O Journal Pacifique é dedicado à distribuição de pesquisas científicas populares em outros idiomas que não o inglês.\",\n",
    "    \"hometext-pl\":\"Witamy w Journal Pacifique. Zapewniamy najnowsze artykuły na temat nauki i technologii. Czasopismo Pacifique poświęcone jest rozpowszechnianiu popularnych badań naukowych w językach innych niż angielski.\",\n",
    "    \"hometext-zh-CN\":\"欢迎来到Journal Pacifique。 我们为您提供有关科学和技术的最新文章。 Journal Pacifique致力于以英语以外的语言分发流行的科学研究。\",\n",
    "    \"hometext-tr\":\"Journal Pacifique'e hoş geldiniz. Bilim ve teknoloji ile ilgili en son makaleleri size sunuyoruz. Journal Pacifique, popüler bilimsel araştırmaların İngilizce dışındaki dillerde dağıtımına adanmıştır.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyhS7v6RUPR4"
   },
   "outputs": [],
   "source": [
    "def url_to_dirfile(url,target_language):\n",
    "\n",
    "    ar_di = sciencedaily_parse_article(url)\n",
    "\n",
    "    htmlx = dic_to_dirfile(ar_di,target_language, elements_dictionary)\n",
    "  \n",
    "    return [htmlx,ar_di]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UciJeDgfqz3r"
   },
   "outputs": [],
   "source": [
    "def get_article_urls_sd():\n",
    "\n",
    "    main = \"https://www.sciencedaily.com/news/top/technology/\"\n",
    "    req = Request(main, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    main_html = webpage.decode(\"utf-8\") \n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    headline  = find_between(main_html, '<h5 class=\"clearfix\"><a href=\"','\">')\n",
    "    \n",
    "    for i in range(12):\n",
    "        main_html = main_html[main_html.index(headline)+len(headline):]\n",
    "        links.append(\"https://www.sciencedaily.com\"+headline)\n",
    "        headline  = find_between(main_html, '<h5 class=\"clearfix\"><a href=\"','\">')\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "un2GoaZampVw"
   },
   "outputs": [],
   "source": [
    "articles = get_article_urls_sd()\n",
    "            \n",
    "languages = [\"es\",\"pt\",\"pl\",\"zh-CN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/02/200228142022.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200311121832.htm',\n",
       " 'https://www.sciencedaily.com/releases/2015/11/151110102147.htm',\n",
       " 'https://www.sciencedaily.com/releases/2019/09/190903134732.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/02/200226110843.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200302153551.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/02/200220141748.htm']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Or4n3Fx01Mu"
   },
   "outputs": [],
   "source": [
    "headlinessofar = []\n",
    "articlessofar  = {}\n",
    "articleurlssofar = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#headlinessofar = load_obj(\"headlinessofar\")\n",
    "#articlessofar = load_obj(\"articlessofar\")\n",
    "#articleurlssofar =load_obj(\"articleurlssofar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refresh already saved articles' html\n",
    "#for when a design change is implemented.\n",
    "import time\n",
    "\n",
    "for article in articlessofar:\n",
    "    tmp = targdic_to_dirfile(articlessofar[article], articlessofar[article][\"lang\"], elements_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "F5uaV7Za0s9w",
    "outputId": "8f041e40-ca70-4c97-882e-9b5d0e2ac936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Antiguos muestra la cáscara del día eran de media hora más corto hace 70 millones de años \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Antigos mostra shell dias foram meia hora mais curta 70 milhões de anos atrás \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Starożytne pokazy powłoki dni były pół godziny krócej 70 milionów lat temu \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "古贝节目天半小时缩短7000万年以前\n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "¿Por qué hay ninguna materia en el universo en absoluto? Un nuevo estudio arroja \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Por que não há qualquer matéria no universo em tudo? Novo estudo lança luz Cientistas \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Dlaczego istnieje materia we wszechświecie w ogóle? Nowe badania rzucają światło \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "为什么会出现在所有宇宙中的任何事情？新研究揭示光\n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Exoplaneta donde llueve hierro descubierto \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Exoplanet onde chove de ferro descoberto \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Egzoplanetą gdzie pada deszcz żelaza odkryte \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "太阳系外行星，其中下雨铁发现，他们怀疑下雨铁\n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "El uso de cobre para prevenir la propagación de virus respiratorios \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Usando cobre para evitar a propagação de vírus respiratórios \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Korzystanie z miedzi, aby zapobiec rozprzestrzenianiu się wirusów oddechowych \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "使用铜，以防止呼吸道病毒的传播\n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Las mascarillas quirúrgicas tan buenos como los respiradores para la gripe y la protección de virus respiratorios \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "máscaras cirúrgicas tão bom como respiradores para gripe e proteção contra vírus respiratório \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Maski chirurgiczne tak dobry jak respiratory na grypę i ochrony antywirusowej oddechowy \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "外科手术口罩不如呼吸器流感和呼吸道病毒防护\n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Un nuevo estudio permite cerebro y las neuronas artificiales que se vinculan a través de Internet \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Novo estudo permite cerebrais e artificiais neurônios para a ligação através da web \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Nowe badanie pozwala mózgowe i sztuczne neurony połączyć się przez Internet \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "新研究使大脑和神经细胞的人工通过Web\n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Para predecir una epidemia, la evolución no puede ser ignorada \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Para prever uma epidemia, a evolução não pode ser ignorado \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Do przewidywania epidemii, ewolucja nie może być ignorowane \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "为了预测流行病，发展也不容忽视\n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "los rendimientos de inteligencia artificial nuevo antibiótico \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "rendimentos de inteligência artificial novo antibiótico \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "Sztuczne plony wywiadowcze nowy antybiotyk \n",
      "headline\n",
      "meta\n",
      "article\n",
      "imgalt\n",
      "人工智能产量新抗生素\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "test_enable = False\n",
    "\n",
    "for article in articles:\n",
    "    if (article not in articleurlssofar) or test_enable:\n",
    "        dic = sciencedaily_parse_article(article)\n",
    "        for target_language in languages:\n",
    "                tmp = dic_to_dirfile(dic,target_language, elements_dictionary)\n",
    "                articlessofar[tmp[2]]=tmp[1]\n",
    "        articleurlssofar.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(headlinessofar,     \"headlinessofar\")\n",
    "save_obj(articlessofar,       \"articlessofar\")\n",
    "save_obj(articleurlssofar, \"articleurlssofar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLMAwueUAB1f"
   },
   "outputs": [],
   "source": [
    "#get most recent 9 articles and get their \"keys\"\n",
    "\n",
    "def form_index(target_language):\n",
    "  \n",
    "    homepage_text=\"welcome my friend we have carpets.\"\n",
    " \n",
    "    html = open(SUBFOLDER + \"jp-index.temp\").read()\n",
    "    \n",
    "    from datetime import date\n",
    "    hoy  = date.today()\n",
    "    d1   = hoy.strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    html = re.sub(r\"\\$\\$date%%\", d1, html)\n",
    "\n",
    "    html = re.sub(r\"\\$\\$home%%\",           elements_dictionary[\"homepage-\"+target_language],           html)\n",
    "    html = re.sub(r\"\\$\\$archive%%\",        elements_dictionary[\"archive-\"+target_language],            html)\n",
    "    html = re.sub(r\"\\$\\$about%%\",          elements_dictionary[\"about us-\"+target_language],           html)\n",
    "    html = re.sub(r\"\\$\\$jp-translation%%\", elements_dictionary[\"journal pacifique-\"+target_language],  html)\n",
    "    html = re.sub(r\"\\$\\$latest-articles%%\",elements_dictionary[\"latest-articles-\"+target_language],    html)\n",
    "    \n",
    "    #save these to elements dictionary\n",
    "    html = re.sub(r\"\\$\\$homepage-text%%\",  elements_dictionary[\"hometext-\"+target_language],    html)\n",
    "    \n",
    "    html = re.sub(r\"\\$\\$target-language%%\",target_language,  html)\n",
    "\n",
    "    asfl = []\n",
    "    for key in articlessofar:\n",
    "        if(articlessofar[key][\"lang\"]==target_language):\n",
    "            asfl.append(key)\n",
    "            \n",
    "    writenum = min(len(asfl),9)\n",
    "    \n",
    "    for i in range (writenum):\n",
    "        j = writenum-i\n",
    "        \n",
    "        key = asfl[i]\n",
    "    \n",
    "        html = re.sub(r\"\\$\\$article-link\"+repr(j)+\"%%\",   articlessofar[key][\"pathfromlang\"][1:],           html)\n",
    "        print(articlessofar[key][\"pathfromlang\"])\n",
    "        html = re.sub(r\"\\$\\$headline\"+repr(j)+\"%%\",           articlessofar[key][\"headline\"],           html)\n",
    "        html = re.sub(r\"\\$\\$headlinemeta\"+repr(j)+\"%%\",           articlessofar[key][\"meta\"],           html)\n",
    "        if(articlessofar[key][\"imgaddr\"][:2]!=\"..\"):\n",
    "            html = re.sub(r\"\\$\\$headline-img\"+repr(j)+\"%%\",      articlessofar[key][\"imgaddr\"],         html)\n",
    "        else:\n",
    "            html = re.sub(r\"\\$\\$headline-img\"+repr(j)+\"%%\",      articlessofar[key][\"imgaddr\"][6:],              html)\n",
    "    \n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7tM1pNHvsRo"
   },
   "outputs": [],
   "source": [
    "def refresh_indices():\n",
    "    for language in languages:\n",
    "\n",
    "        htmlx = form_index(language)\n",
    "        path  = language +\"/index.html\"\n",
    "        \n",
    "        import os\n",
    "        cmd = \"mkdir -p \"+SUBFOLDER+language\n",
    "        os.system(cmd)\n",
    "        cmd = \"touch \"+SUBFOLDER+path\n",
    "        os.system(cmd)\n",
    "        \n",
    "        f= open(SUBFOLDER+path,\"w+\")\n",
    "        f.write(htmlx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiGEb3gaxBDH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/posts/2020-03-15/Ancient-shell-shows-days-were-half-hour-shorter-70-million-years-ago-es.html\n",
      "/posts/2020-03-15/Why-is-there-any-matter-in-the-universe-at-all--New-study-sheds-light-es.html\n",
      "/posts/2020-03-15/Exoplanet-where-it-rains-iron-discovered-es.html\n",
      "/posts/2020-03-15/Using-copper-to-prevent-the-spread-of-respiratory-viruses-es.html\n",
      "/posts/2020-03-15/Surgical-masks-as-good-as-respirators-for-flu-and-respiratory-virus-protection-es.html\n",
      "/posts/2020-03-15/New-study-allows-brain-and-artificial-neurons-to-link-up-over-the-web-es.html\n",
      "/posts/2020-03-15/To-predict-an-epidemic--evolution-can-t-be-ignored-es.html\n",
      "/posts/2020-03-15/Artificial-intelligence-yields-new-antibiotic-es.html\n",
      "/posts/2020-03-15/Ancient-shell-shows-days-were-half-hour-shorter-70-million-years-ago-pt.html\n",
      "/posts/2020-03-15/Why-is-there-any-matter-in-the-universe-at-all--New-study-sheds-light-pt.html\n",
      "/posts/2020-03-15/Exoplanet-where-it-rains-iron-discovered-pt.html\n",
      "/posts/2020-03-15/Using-copper-to-prevent-the-spread-of-respiratory-viruses-pt.html\n",
      "/posts/2020-03-15/Surgical-masks-as-good-as-respirators-for-flu-and-respiratory-virus-protection-pt.html\n",
      "/posts/2020-03-15/New-study-allows-brain-and-artificial-neurons-to-link-up-over-the-web-pt.html\n",
      "/posts/2020-03-15/To-predict-an-epidemic--evolution-can-t-be-ignored-pt.html\n",
      "/posts/2020-03-15/Artificial-intelligence-yields-new-antibiotic-pt.html\n",
      "/posts/2020-03-15/Ancient-shell-shows-days-were-half-hour-shorter-70-million-years-ago-pl.html\n",
      "/posts/2020-03-15/Why-is-there-any-matter-in-the-universe-at-all--New-study-sheds-light-pl.html\n",
      "/posts/2020-03-15/Exoplanet-where-it-rains-iron-discovered-pl.html\n",
      "/posts/2020-03-15/Using-copper-to-prevent-the-spread-of-respiratory-viruses-pl.html\n",
      "/posts/2020-03-15/Surgical-masks-as-good-as-respirators-for-flu-and-respiratory-virus-protection-pl.html\n",
      "/posts/2020-03-15/New-study-allows-brain-and-artificial-neurons-to-link-up-over-the-web-pl.html\n",
      "/posts/2020-03-15/To-predict-an-epidemic--evolution-can-t-be-ignored-pl.html\n",
      "/posts/2020-03-15/Artificial-intelligence-yields-new-antibiotic-pl.html\n",
      "/posts/2020-03-15/Ancient-shell-shows-days-were-half-hour-shorter-70-million-years-ago-zh-CN.html\n",
      "/posts/2020-03-15/Why-is-there-any-matter-in-the-universe-at-all--New-study-sheds-light-zh-CN.html\n",
      "/posts/2020-03-15/Exoplanet-where-it-rains-iron-discovered-zh-CN.html\n",
      "/posts/2020-03-15/Using-copper-to-prevent-the-spread-of-respiratory-viruses-zh-CN.html\n",
      "/posts/2020-03-15/Surgical-masks-as-good-as-respirators-for-flu-and-respiratory-virus-protection-zh-CN.html\n",
      "/posts/2020-03-15/New-study-allows-brain-and-artificial-neurons-to-link-up-over-the-web-zh-CN.html\n",
      "/posts/2020-03-15/To-predict-an-epidemic--evolution-can-t-be-ignored-zh-CN.html\n",
      "/posts/2020-03-15/Artificial-intelligence-yields-new-antibiotic-zh-CN.html\n"
     ]
    }
   ],
   "source": [
    "refresh_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKuJ6WK3qz8Z"
   },
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "commit_message = \"Regular update\"\n",
    "cmdr = SUBFOLDER + ' git add --all; git commit -m \"' + commit_message  + '\"; git push; cd ..'\n",
    "if SUBFOLDER != '':\n",
    "    cmdl = 'cd ' \n",
    "else:\n",
    "    cmdl = ''\n",
    "cmd = cmdl + cmdr\n",
    "os.system(cmd)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mo1ubDJ8qz_P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKfQHt50q0Dx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIHb6cRlepob"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXLuwSPjXGx-"
   },
   "outputs": [],
   "source": [
    "\"\"\"#parse given url (for quanta mag) and return translation of the article.\n",
    "\n",
    "def quanta_parser(url, target_language='tr'):\n",
    "  site = url\n",
    "  html_string = \"\"\n",
    "  tagged_w_ps = \"\"\n",
    "\n",
    "  # parse html\n",
    "\n",
    "  from mechanize import Browser\n",
    "  br = Browser()\n",
    "  br.set_handle_robots(False) #illegal\n",
    "  br.open(site)\n",
    "  html = br.response().readlines()\n",
    "  for line in html:\n",
    "    html_string = html_string+line.decode(\"utf-8\") \n",
    "    \n",
    "    \n",
    "  # get text in the main section under <section> tag.  \n",
    "  \n",
    "  for item in html_string.split(\"</section>\"):\n",
    "    if \"<section>\" in item:\n",
    "      tagged_w_ps = tagged_w_ps + item[ item.find(\"<section>\")+len(\"<section>\") : ]\n",
    "      \n",
    "  \n",
    "  # get text in the paragraph tags (<p>).\n",
    "  \n",
    "  for item in html_string.split(\"</p>\"):\n",
    "    if \"<p>\" in item:\n",
    "      tagged_w_ps = tagged_w_ps + (item[ item.find(\"<p>\")+len(\"<p>\") : ]+\"ğ\")\n",
    "      \n",
    "      \n",
    "  # remove alt texts.\n",
    "  tagged_w_ps = re.sub(r\" *\\..*(Quanta Magazine)\", \".\", tagged_w_ps)\n",
    "      \n",
    "  \n",
    "  # take link tags out of the way and edit custom characters.\n",
    "  # TODO: expand this list for future articles with other unicode characters.\n",
    "      \n",
    "  newstr = re.sub(r\"(<a href).*?(\\\">)\", \"\", tagged_w_ps)\n",
    "  newstr = re.sub(r\"(\\\\u2018)\", \"\\\"\", newstr)\n",
    "  newstr = re.sub(r\"(\\\\u2019)\", \"\\\"\", newstr)\n",
    "  newstr = re.sub(r\"(\\xa0)\", \" \", newstr)\n",
    "  newstr = re.sub(r\"(&#8217)\", \"\\'\", newstr)\n",
    "  newstr = re.sub(r\"(</a>)\", \"\", newstr)\n",
    "  newstr = re.sub(r\"\\.<.*?(quote_attribution).*?<p>\", \" says \", newstr)\n",
    "  newstr = re.sub(r\"(<p>)\", \"\", newstr)\n",
    "  newstr = re.sub(r\"(<).*(>)\", \".\", newstr)\n",
    "  newstr = re.sub(r\"(\\\",\\\"caption).*}}\", \"\", newstr)\n",
    "  \n",
    "  \n",
    "  # convert custom newline tag to newline\n",
    "  newstr = re.sub(r\"ğ+\", \"\\n\\n\", newstr)\n",
    "  \n",
    "  # translate the text to your target language.\n",
    "  \n",
    "  translation = translator.translate(newstr, dest=target_language).text\n",
    "  \n",
    "  return translation\n",
    "  \n",
    "  \"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TRANSLATE2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
