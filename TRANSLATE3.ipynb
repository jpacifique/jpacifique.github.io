{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZJsH6GX3LcU"
   },
   "source": [
    "## \"journal pacifique\" web bot\n",
    "parses sources for new articles, translates them into target languages [\"es\",\"pt\",\"pl\",\"zh-CN\"]. Wraps it up in a simple html document and writes it out to the appropriate directory.\n",
    "\n",
    "#### TO DO TECHNICAL:\n",
    "\n",
    "*   save the articles dictionary as a json or something (__done__)\n",
    "*   *   (dic backup for later design changes) important\n",
    "*   implement an automatic parser (__done(sd)__)\n",
    "*   detect and highlight keywords\n",
    "*   automatic git push (__done__ but pass cache time should be increased)\n",
    "*   run the script on an aws server\n",
    "*   maybe an open source grammar rating mechanism?\n",
    "*   don't send a separate request to the website for every translation (__done__)\n",
    "*   same pages shouldn't be crawled twice. Even if they are crawled they shouldn't be overwritten. (__done__)\n",
    "*   implement waiting if requests exhaust limit\n",
    "*   also implement a timeout. maybe take articles into a queue.\n",
    "*   re-read SD's [terms](https://www.sciencedaily.com/terms.htm) on reproduction. (especially on images)\n",
    "*   take the bot 1 directory up so it won't be pushed into the public repo. (__done__)\n",
    "\n",
    "#### TO DO GRAPHICAL\n",
    "*   responsive text, currently changes size by page??\n",
    "*   resizable design (__done__)\n",
    "*   mobile-friendly design\n",
    "*   *   [media queries for different device widths](https://stackoverflow.com/questions/16387400/getting-the-right-font-size-on-every-mobile-device)\n",
    "*   subscribe and social media block\n",
    "\n",
    "\n",
    "#### POSSIBLE SOURCES\n",
    "*   [the conversation](https://theconversation.com/uk/republishing-guidelines)\n",
    "*   *   approval needed for translation\n",
    "*   \n",
    "\n",
    "NOTE: google transalte tokens refresh every hour (?)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "am1FAv62IYw-",
    "outputId": "a6430627-a1b7-4973-b011-c8db27c4d236"
   },
   "outputs": [],
   "source": [
    "#for colab\n",
    "#!pip install googletrans\n",
    "#!pip install mechanize\n",
    "\n",
    "## Character limit: 15K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#cmd = \"cd cemreefe.github.io; git pull; cd ..\"\n",
    "#os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcmCkaflIcmH"
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import re\n",
    "\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSj3rydyWNvr"
   },
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gg1MHPIoPIKG"
   },
   "outputs": [],
   "source": [
    "def string_strip(s):\n",
    "    return re.sub(r\"[^A-Za-z0-9]\", \"-\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cFY6cTtOwLM"
   },
   "outputs": [],
   "source": [
    "#def sciencedaily_parse_links():\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-z6efhTaAfI"
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from random import randrange\n",
    "\n",
    "def sciencedaily_parse_article(url):\n",
    "  \n",
    "    site = url\n",
    "    html_string = \"\"\n",
    "    tagged_w_ps = \"\"\n",
    "\n",
    "    # parse html\n",
    "\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    html_string = webpage.decode(\"utf-8\") \n",
    "  \n",
    "    # get info\n",
    "  \n",
    "    # get the headline\n",
    "\n",
    "    headline = find_between(html_string, '<h1 id=\"headline\" class=\"headline\">','</h1>')\n",
    "  \n",
    "    # get the meta\n",
    "  \n",
    "    meta     = find_between(html_string, '<dd id=\"abstract\">','</dd>')\n",
    "  \n",
    "    # get the img address\n",
    "  \n",
    "    imgaddr  = find_between(html_string, 'center-block\" src=\"','\"')\n",
    "    if(imgaddr!=\"\"):\n",
    "        imgaddr  = 'https://www.sciencedaily.com'+imgaddr\n",
    "    else:\n",
    "        imgaddr  = '../../../img/dummy'+repr(randrange(20))+'.jpeg'\n",
    "  \n",
    "    # get the img alt\n",
    "  \n",
    "    imgalt   = find_between(html_string, '<div class=\"photo-caption\">','</div>')\n",
    "    cred     = '\\t'+find_between(html_string, '<div class=\"photo-credit\"><em>','</div>')\n",
    "  \n",
    "    # get the citation\n",
    "  \n",
    "    citation = find_between(html_string, '<div role=\"tabpanel\" class=\"tab-pane active\" id=\"citation_mla\">','</div>')\n",
    "  \n",
    "    \n",
    "    # get the article itself\n",
    "\n",
    "    article  = find_between(html_string, '<div id=\"text\">','</div>')\n",
    "    article  = re.sub(r\"<p>\" ,     \"\", article)\n",
    "    article  = re.sub(r\"</p>\", \"\\n\", article)\n",
    "  \n",
    "    # get the href at the end if it exists\n",
    "    href_tab = \"\"\n",
    "    href_tab = href_tab + find_between(article, '<a href','</a>')\n",
    "  \n",
    "    # delete the href and useless tags from the article\n",
    "  \n",
    "    article  = re.sub(r\"<a href.*?</a>\", \"\", article)\n",
    "    article  = re.sub(r\"<.*?>\", \"\",          article)\n",
    "    article  = re.sub(r\" -- \", \", \",         article)\n",
    "  \n",
    "    # special characters, this list will expand.\n",
    "  \n",
    "    #article  = re.sub(r\"&uuml;\", \"ü\",         article)\n",
    "    #article  = re.sub(r\"&ouml;\", \"ö\",         article)\n",
    "    #article  = re.sub(r\"&eacute;\", \"e\",         article)\n",
    "    # new sltn:\n",
    "    import html\n",
    "    article   = html.unescape(article)\n",
    "      \n",
    "            #y                    #y                                #y                                                       #y                  \n",
    "    return {\"headline\": headline, \"meta\": meta, \"imgaddr\": imgaddr, \"imgalt\": imgalt, \"imgcredit\":cred,\"citation\": citation, \"article\": article, \"href\": href_tab}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9bqtXa0mCjf"
   },
   "outputs": [],
   "source": [
    "def get_translation(article_dictionary,target_language):\n",
    "    big_string= \"\"\n",
    "    translations = {}\n",
    "    token_in = \"(#@)\"\n",
    "    if (target_language==\"zh-CN\"):\n",
    "        token_out = \"（＃@）\"\n",
    "    else: token_out=\"(# @)\"\n",
    "      \n",
    "    \n",
    "    #sırası karışmasın!! keylerden array oluştur, arrayi itere et.\n",
    "    keys = [\"headline\",\"meta\",\"article\",\"imgalt\"]\n",
    "    \n",
    "    for key in keys:\n",
    "        print(key)\n",
    "        if big_string != \"\":\n",
    "            big_string = big_string + token_in + article_dictionary[key]\n",
    "        else:\n",
    "            big_string = article_dictionary[key]\n",
    "            \n",
    "    big_translated_string = translator.translate(big_string,dest=target_language).text\n",
    "    \n",
    "    translations_array    = big_translated_string.split(token_out)\n",
    "   \n",
    "    #print(big_translated_string)\n",
    "    #print(len(translations_array),translations_array)\n",
    "    \n",
    "    for i in range (4):\n",
    "        translations[keys[i]]=translations_array[i]\n",
    "    translations[\"imgaddr\"]=article_dictionary[\"imgaddr\"]\n",
    "    translations[\"citation\"]=article_dictionary[\"citation\"]\n",
    "    translations[\"imgcredit\"]=article_dictionary[\"imgcredit\"]\n",
    "    translations[\"href\"]=article_dictionary[\"href\"]\n",
    "    translations[\"org-headline\"]=article_dictionary[\"headline\"]\n",
    "    translations[\"lang\"]=target_language\n",
    "    \n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcFIKve4csNY"
   },
   "outputs": [],
   "source": [
    ";# deprecated\n",
    "def get_language_dictionary(target_language):\n",
    "    ld = {}\n",
    "    words = [\"journal pacifique\", \"homepage\", \"archive\", \"about us\", \"source:\"]\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        ld[word] = translator.translate(word, dest=target_language).text\n",
    "    return ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSvSx7ACTufo"
   },
   "outputs": [],
   "source": [
    "def html_from_dictionary(translated_dictionary, target_language, language_dictionary): # translated & language could be just one dictionary this was stupid.\n",
    "  \n",
    "    from datetime import date\n",
    "    hoy  = date.today()\n",
    "    d1   = hoy.strftime(\"%d/%m/%Y\")\n",
    "  \n",
    "    html = open(SUBFOLDER + \"jp.temp\").read()\n",
    "\n",
    "    html_article = re.sub(r\"\\n\\n\", \"</p>\\n\\n<p>\",   translated_dictionary[\"article\"])\n",
    "    html_article = \"<p>\" + html_article + \"</p>\"\n",
    "  \n",
    "    #print(translated_dictionary)\n",
    "    #print(language_dictionary)\n",
    "    print(translated_dictionary[\"headline\"])\n",
    "  \n",
    "    html = re.sub(r\"\\$\\$article-title%%\",  translated_dictionary[\"headline\"],         html)\n",
    "    html = re.sub(r\"\\$\\$img-alt%%\",        translated_dictionary[\"imgalt\"],           html)\n",
    "    html = re.sub(r\"\\$\\$article-meta%%\",   translated_dictionary[\"meta\"],             html)\n",
    "    html = re.sub(r\"\\$\\$source%%\",         translated_dictionary[\"citation\"],            html)\n",
    "    html = re.sub(r\"\\$\\$img.jpg%%\",        translated_dictionary[\"imgaddr\"],             html)\n",
    "    html = re.sub(r\"\\$\\$article-text%%\",   html_article                  ,            html)\n",
    "  \n",
    "    html = re.sub(r\"\\$\\$home%%\",           language_dictionary[\"homepage-\"+target_language],           html)\n",
    "    html = re.sub(r\"\\$\\$archive%%\",        language_dictionary[\"archive-\"+target_language],            html)\n",
    "    html = re.sub(r\"\\$\\$about%%\",          language_dictionary[\"about us-\"+target_language],           html)\n",
    "    html = re.sub(r\"\\$\\$jp-translation%%\", language_dictionary[\"journal pacifique-\"+target_language],  html)\n",
    "    html = re.sub(r\"\\$\\$taken-from%%\",     language_dictionary[\"source:-\"+target_language],            html)\n",
    "    html = re.sub(r\"\\$\\$target-languages%%\",     target_language,            html)\n",
    "  \n",
    "    html = re.sub(r\"\\$\\$imgcredit%%\",       translated_dictionary[\"imgcredit\"],          html)\n",
    "  \n",
    "    html = re.sub(r\"\\$\\$href%%\",           translated_dictionary[\"href\"],            html)\n",
    "  \n",
    "    html = re.sub(r\"-- \", \",\",  html)\n",
    " \n",
    "    html = re.sub(r\"\\$\\$date%%\", d1, html)\n",
    "  \n",
    "    #html = re.sub(r'</head>', '<style> p { text-indent: 30px;} </style>\\n\\t</head>', html)\n",
    "  \n",
    "    return html\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8_T668O03h9"
   },
   "outputs": [],
   "source": [
    "def getnewpath(translated_dict):\n",
    "  \n",
    "    from datetime import date\n",
    "    hoy  = date.today()\n",
    "    date = hoy.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    translated_dict[\"date\"]=date\n",
    "  \n",
    "    urlheadline = string_strip(translated_dict[\"org-headline\"])+\"-\"+target_language\n",
    "    postsfolder = \"/posts/\"+date+\"/\"\n",
    "    newpathaddr = target_language+postsfolder\n",
    "    newheadline = newpathaddr+urlheadline+\".html\"\n",
    "    newheadlang = postsfolder+urlheadline+\".html\"\n",
    "    \n",
    "    translated_dict[\"pathfromhome\"]=newheadline\n",
    "    translated_dict[\"pathfromlang\"]=newheadlang\n",
    "    \n",
    "    # this was implemented for short headlines (~10char)\n",
    "    #if newheadline in headlinessofar:\n",
    "    #    c=2\n",
    "    #    while (newheadline[:-5]+repr(c)+newheadline[-5:] in headlinessofar):\n",
    "    #        c+=1\n",
    "    #    newheadline = newheadline[:-5]+repr(c)+newheadline[-5:]\n",
    "    \n",
    "    headlinessofar.append(newheadline)\n",
    "    return [newheadline,newpathaddr,urlheadline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBFOLDER = \"cemreefe.github.io/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HD6jZB1jmpVe"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(SUBFOLDER+'obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(SUBFOLDER+'obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5dlG_rpmpVm"
   },
   "outputs": [],
   "source": [
    "def dic_to_dirfile(dic,target_language, elements_dictionary):\n",
    "    \n",
    "    #we used to translate all elements at every step, now it is automated due to\n",
    "    #google translate's quota restrictions.\n",
    "    \n",
    "    #zh_ld = get_language_dictionary(target_language)\n",
    "\n",
    "    tr_di = get_translation(dic,target_language)\n",
    "    htmlx = html_from_dictionary(tr_di, target_language, elements_dictionary)\n",
    "  \n",
    "    newdirs = getnewpath(tr_di)\n",
    "  \n",
    "    import os\n",
    "    cmd = \"mkdir -p \"+SUBFOLDER+newdirs[1]\n",
    "    os.system(cmd)\n",
    "  \n",
    "    f= open(SUBFOLDER+newdirs[0],\"w+\")\n",
    "    f.write(htmlx)\n",
    "\n",
    "    return [htmlx,tr_di,newdirs[2],newdirs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targdic_to_dirfile(tr_di,target_language, elements_dictionary):\n",
    "    \n",
    "    htmlx = html_from_dictionary(tr_di, target_language, elements_dictionary)\n",
    "  \n",
    "    newdirs = getnewpath(tr_di)\n",
    "  \n",
    "    import os\n",
    "    cmd = \"mkdir -p \"+SUBFOLDER+newdirs[1]\n",
    "    os.system(cmd)\n",
    "  \n",
    "    f= open(SUBFOLDER+newdirs[0],\"w+\")\n",
    "    f.write(htmlx)\n",
    "    \n",
    "    print(SUBFOLDER+newdirs[0])\n",
    "  \n",
    "    return [htmlx,tr_di,newdirs[2],newdirs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7W8jsdDvoxH"
   },
   "outputs": [],
   "source": [
    "elements_dictionary = {\n",
    "    \"journal pacifique-es\":\"periódico pacífico\", \n",
    "    \"homepage-es\":\"página principal\", \n",
    "    \"archive-es\":\"archivo\",\n",
    "    \"about us-es\":\"sobre nosotros\",\n",
    "    \"source:-es\":\"fuente:\",\n",
    "    \"journal pacifique-pt\":\"jornal pacífico\", \n",
    "    \"homepage-pt\":\"pagina inicial\", \n",
    "    \"archive-pt\":\"arquivo\",\n",
    "    \"about us-pt\":\"sobre nós\",\n",
    "    \"source:-pt\":\"fonte:\",\n",
    "    \"journal pacifique-tr\":\"barışçıl gazete\", \n",
    "    \"homepage-tr\":\"anasayfa\", \n",
    "    \"archive-tr\":\"arşiv\",\n",
    "    \"about us-tr\":\"hakkında\",\n",
    "    \"source:-tr\":\"kaynakça:\",\n",
    "    \"journal pacifique-zh-CN\":\"和平的报纸\", \n",
    "    \"homepage-zh-CN\":\"主页\", \n",
    "    \"archive-zh-CN\":\"档案\",\n",
    "    \"about us-zh-CN\":\"关于我们\",\n",
    "    \"source:-zh-CN\":\"资源\",\n",
    "    \"journal pacifique-pl\":\"spokojna gazeta\", \n",
    "    \"homepage-pl\":\"strona główna\", \n",
    "    \"archive-pl\":\"archiwum\",\n",
    "    \"about us-pl\":\"o nas\",\n",
    "    \"source:-pl\":\"źródło\",\n",
    "    \"latest-articles-es\":\"Últimos artículos\",\n",
    "    \"latest-articles-pt\":\"Artigos Mais Recentes\",\n",
    "    \"latest-articles-pl\":\"Ostatnie artykuły\",\n",
    "    \"latest-articles-zh-CN\":\"最新的文章\",\n",
    "    \"latest_articles-tr\":\"En yeni makaleler\",\n",
    "    \"hometext-es\":\"Bienvenido a Journal Pacifique. Le proporcionamos los últimos artículos sobre ciencia y tecnología. Journal Pacifique se dedica a la distribución de investigaciones científicas populares en otros idiomas además del inglés.\",\n",
    "    \"hometext-pt\":\"Bem-vindo ao Journal Pacifique. Fornecemos os artigos mais recentes sobre ciência e tecnologia. O Journal Pacifique é dedicado à distribuição de pesquisas científicas populares em outros idiomas que não o inglês.\",\n",
    "    \"hometext-pl\":\"Witamy w Journal Pacifique. Zapewniamy najnowsze artykuły na temat nauki i technologii. Czasopismo Pacifique poświęcone jest rozpowszechnianiu popularnych badań naukowych w językach innych niż angielski.\",\n",
    "    \"hometext-zh-CN\":\"欢迎来到Journal Pacifique。 我们为您提供有关科学和技术的最新文章。 Journal Pacifique致力于以英语以外的语言分发流行的科学研究。\",\n",
    "    \"hometext-tr\":\"Journal Pacifique'e hoş geldiniz. Bilim ve teknoloji ile ilgili en son makaleleri size sunuyoruz. Journal Pacifique, popüler bilimsel araştırmaların İngilizce dışındaki dillerde dağıtımına adanmıştır.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyhS7v6RUPR4"
   },
   "outputs": [],
   "source": [
    "def url_to_dirfile(url,target_language):\n",
    "\n",
    "    ar_di = sciencedaily_parse_article(url)\n",
    "\n",
    "    htmlx = dic_to_dirfile(ar_di,target_language, elements_dictionary)\n",
    "  \n",
    "    return [htmlx,ar_di]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UciJeDgfqz3r"
   },
   "outputs": [],
   "source": [
    "def get_article_urls_sd():\n",
    "\n",
    "    main = \"https://www.sciencedaily.com/news/top/technology/\"\n",
    "    req = Request(main, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    main_html = webpage.decode(\"utf-8\") \n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    headline  = find_between(main_html, '<h5 class=\"clearfix\"><a href=\"','\">')\n",
    "    \n",
    "    for i in range(12):\n",
    "        main_html = main_html[main_html.index(headline)+len(headline):]\n",
    "        links.append(\"https://www.sciencedaily.com\"+headline)\n",
    "        headline  = find_between(main_html, '<h5 class=\"clearfix\"><a href=\"','\">')\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "un2GoaZampVw"
   },
   "outputs": [],
   "source": [
    "articles = get_article_urls_sd()\n",
    "            \n",
    "languages = [\"es\",\"pt\",\"pl\",\"zh-CN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200309135410.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/02/200228142022.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200311121832.htm',\n",
       " 'https://www.sciencedaily.com/releases/2015/11/151110102147.htm',\n",
       " 'https://www.sciencedaily.com/releases/2019/09/190903134732.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/02/200226110843.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/03/200302153551.htm',\n",
       " 'https://www.sciencedaily.com/releases/2020/02/200220141748.htm']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Or4n3Fx01Mu"
   },
   "outputs": [],
   "source": [
    "headlinessofar = []\n",
    "articlessofar  = {}\n",
    "articleurlssofar = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cemreefe.github.io/obj/headlinessofar.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-614774b5993a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mheadlinessofar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"headlinessofar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0marticlessofar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"articlessofar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marticleurlssofar\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"articleurlssofar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-635e0bcc5596>\u001b[0m in \u001b[0;36mload_obj\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBFOLDER\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'obj/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cemreefe.github.io/obj/headlinessofar.pkl'"
     ]
    }
   ],
   "source": [
    "headlinessofar = load_obj(\"headlinessofar\")\n",
    "articlessofar = load_obj(\"articlessofar\")\n",
    "articleurlssofar =load_obj(\"articleurlssofar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#refresh already saved articles' html\n",
    "#for when a design change is implemented.\n",
    "import time\n",
    "\n",
    "for article in articlessofar:\n",
    "    tmp = targdic_to_dirfile(articlessofar[article], articlessofar[article][\"lang\"], elements_dictionary)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "F5uaV7Za0s9w",
    "outputId": "8f041e40-ca70-4c97-882e-9b5d0e2ac936"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "test_enable = False\n",
    "\n",
    "for article in articles:\n",
    "    if (article not in articleurlssofar) or test_enable:\n",
    "        dic = sciencedaily_parse_article(article)\n",
    "        for target_language in languages:\n",
    "                tmp = dic_to_dirfile(dic,target_language, elements_dictionary)\n",
    "                articlessofar[tmp[2]]=tmp[1]\n",
    "        articleurlssofar.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(headlinessofar,     \"headlinessofar\")\n",
    "save_obj(articlessofar,       \"articlessofar\")\n",
    "save_obj(articleurlssofar, \"articleurlssofar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLMAwueUAB1f"
   },
   "outputs": [],
   "source": [
    "#get most recent 9 articles and get their \"keys\"\n",
    "\n",
    "def form_index(target_language):\n",
    "  \n",
    "    homepage_text=\"welcome my friend we have carpets.\"\n",
    " \n",
    "    html = open(SUBFOLDER + \"jp-index.temp\").read()\n",
    "    \n",
    "    from datetime import date\n",
    "    hoy  = date.today()\n",
    "    d1   = hoy.strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    html = re.sub(r\"\\$\\$date%%\", d1, html)\n",
    "\n",
    "    html = re.sub(r\"\\$\\$home%%\",           elements_dictionary[\"homepage-\"+target_language],           html)\n",
    "    html = re.sub(r\"\\$\\$archive%%\",        elements_dictionary[\"archive-\"+target_language],            html)\n",
    "    html = re.sub(r\"\\$\\$about%%\",          elements_dictionary[\"about us-\"+target_language],           html)\n",
    "    html = re.sub(r\"\\$\\$jp-translation%%\", elements_dictionary[\"journal pacifique-\"+target_language],  html)\n",
    "    html = re.sub(r\"\\$\\$latest-articles%%\",elements_dictionary[\"latest-articles-\"+target_language],    html)\n",
    "    \n",
    "    #save these to elements dictionary\n",
    "    html = re.sub(r\"\\$\\$homepage-text%%\",  elements_dictionary[\"hometext-\"+target_language],    html)\n",
    "    \n",
    "    html = re.sub(r\"\\$\\$target-language%%\",target_language,  html)\n",
    "\n",
    "    asfl = []\n",
    "    for key in articlessofar:\n",
    "        if(articlessofar[key][\"lang\"]==target_language):\n",
    "            asfl.append(key)\n",
    "    \n",
    "    for i in range (min(len(asfl),9)):\n",
    "        j=i+1\n",
    "        key = asfl[i]\n",
    "    \n",
    "        html = re.sub(r\"\\$\\$article-link\"+repr(j)+\"%%\",   articlessofar[key][\"pathfromlang\"][1:],           html)\n",
    "        print(articlessofar[key][\"pathfromlang\"])\n",
    "        html = re.sub(r\"\\$\\$headline\"+repr(j)+\"%%\",           articlessofar[key][\"headline\"],           html)\n",
    "        html = re.sub(r\"\\$\\$headlinemeta\"+repr(j)+\"%%\",           articlessofar[key][\"meta\"],           html)\n",
    "        if(articlessofar[key][\"imgaddr\"][:2]!=\"..\"):\n",
    "            html = re.sub(r\"\\$\\$headline-img\"+repr(j)+\"%%\",      articlessofar[key][\"imgaddr\"],         html)\n",
    "        else:\n",
    "            html = re.sub(r\"\\$\\$headline-img\"+repr(j)+\"%%\",      articlessofar[key][\"imgaddr\"][6:],              html)\n",
    "    \n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7tM1pNHvsRo"
   },
   "outputs": [],
   "source": [
    "def refresh_indices():\n",
    "    for language in languages:\n",
    "\n",
    "        htmlx = form_index(language)\n",
    "        path  = language +\"/index.html\"\n",
    "        \n",
    "        import os\n",
    "        cmd = \"mkdir -p \"+SUBFOLDER+language\n",
    "        os.system(cmd)\n",
    "        cmd = \"touch \"+SUBFOLDER+path\n",
    "        os.system(cmd)\n",
    "        \n",
    "        f= open(SUBFOLDER+path,\"w+\")\n",
    "        f.write(htmlx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiGEb3gaxBDH"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'refresh_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-234e06b424b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrefresh_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'refresh_indices' is not defined"
     ]
    }
   ],
   "source": [
    "refresh_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKuJ6WK3qz8Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "commit_message = \"Regular update\"\n",
    "cmd = 'cd cemreefe.github.io; git add --all; git commit -m \"' + commit_message  + '\"; git push; cd ..'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mo1ubDJ8qz_P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKfQHt50q0Dx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIHb6cRlepob"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXLuwSPjXGx-"
   },
   "outputs": [],
   "source": [
    "\"\"\"#parse given url (for quanta mag) and return translation of the article.\n",
    "\n",
    "def quanta_parser(url, target_language='tr'):\n",
    "  site = url\n",
    "  html_string = \"\"\n",
    "  tagged_w_ps = \"\"\n",
    "\n",
    "  # parse html\n",
    "\n",
    "  from mechanize import Browser\n",
    "  br = Browser()\n",
    "  br.set_handle_robots(False) #illegal\n",
    "  br.open(site)\n",
    "  html = br.response().readlines()\n",
    "  for line in html:\n",
    "    html_string = html_string+line.decode(\"utf-8\") \n",
    "    \n",
    "    \n",
    "  # get text in the main section under <section> tag.  \n",
    "  \n",
    "  for item in html_string.split(\"</section>\"):\n",
    "    if \"<section>\" in item:\n",
    "      tagged_w_ps = tagged_w_ps + item[ item.find(\"<section>\")+len(\"<section>\") : ]\n",
    "      \n",
    "  \n",
    "  # get text in the paragraph tags (<p>).\n",
    "  \n",
    "  for item in html_string.split(\"</p>\"):\n",
    "    if \"<p>\" in item:\n",
    "      tagged_w_ps = tagged_w_ps + (item[ item.find(\"<p>\")+len(\"<p>\") : ]+\"ğ\")\n",
    "      \n",
    "      \n",
    "  # remove alt texts.\n",
    "  tagged_w_ps = re.sub(r\" *\\..*(Quanta Magazine)\", \".\", tagged_w_ps)\n",
    "      \n",
    "  \n",
    "  # take link tags out of the way and edit custom characters.\n",
    "  # TODO: expand this list for future articles with other unicode characters.\n",
    "      \n",
    "  newstr = re.sub(r\"(<a href).*?(\\\">)\", \"\", tagged_w_ps)\n",
    "  newstr = re.sub(r\"(\\\\u2018)\", \"\\\"\", newstr)\n",
    "  newstr = re.sub(r\"(\\\\u2019)\", \"\\\"\", newstr)\n",
    "  newstr = re.sub(r\"(\\xa0)\", \" \", newstr)\n",
    "  newstr = re.sub(r\"(&#8217)\", \"\\'\", newstr)\n",
    "  newstr = re.sub(r\"(</a>)\", \"\", newstr)\n",
    "  newstr = re.sub(r\"\\.<.*?(quote_attribution).*?<p>\", \" says \", newstr)\n",
    "  newstr = re.sub(r\"(<p>)\", \"\", newstr)\n",
    "  newstr = re.sub(r\"(<).*(>)\", \".\", newstr)\n",
    "  newstr = re.sub(r\"(\\\",\\\"caption).*}}\", \"\", newstr)\n",
    "  \n",
    "  \n",
    "  # convert custom newline tag to newline\n",
    "  newstr = re.sub(r\"ğ+\", \"\\n\\n\", newstr)\n",
    "  \n",
    "  # translate the text to your target language.\n",
    "  \n",
    "  translation = translator.translate(newstr, dest=target_language).text\n",
    "  \n",
    "  return translation\n",
    "  \n",
    "  \"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TRANSLATE2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
